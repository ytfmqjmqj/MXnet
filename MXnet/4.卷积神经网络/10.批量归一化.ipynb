{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 通过autograd来判断当前模式为训练模式或预测模式\n",
    "    if not autograd.is_training():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(axis = 0)\n",
    "            var = ((X - mean) ** 2).mean(axis = 0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上(axis = 1)的均值和方差。这里我们需要\n",
    "            # 保持X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(axis = (0, 2, 3), keepdims = True)\n",
    "            var = ((X - mean) ** 2).mean(axis = (0, 2, 3), keepdims = True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta # 拉升和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "自定义一个BatchNorm层\n",
    "'''\n",
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和和迭代的拉升和偏移参数，分别初始化成0和1\n",
    "        self.gamma = self.params.get('gamma', shape = shape, init = init.One())\n",
    "        self.beta = self.params.get('beta', shape = shape, init = init.Zero())\n",
    "        # 不参与求梯度和迭代的变量，全在CPU上初始化成0\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 如果X不在CPU上，将moving_mean和moving_var复制到X所在设备上\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "            self.moving_var, eps = 1e-5, momentum = 0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用批量归一化层的LeNet\n",
    "'''\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size = 5),\n",
    "       BatchNorm(6, num_dims = 4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size = 2, strides = 2),\n",
    "       nn.Conv2D(16, kernel_size = 5),\n",
    "       BatchNorm(16, num_dims = 4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size = 2, strides = 2),\n",
    "       nn.Dense(120),\n",
    "       BatchNorm(120, num_dims = 2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(84),\n",
    "       BatchNorm(84, num_dims = 2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 0.6807, train acc 0.756, test acc 0.729, time 16.7 sec\n",
      "epoch 2, loss 0.3950, train acc 0.857, test acc 0.862, time 16.0 sec\n",
      "epoch 3, loss 0.3493, train acc 0.874, test acc 0.881, time 15.5 sec\n",
      "epoch 4, loss 0.3250, train acc 0.882, test acc 0.844, time 15.6 sec\n",
      "epoch 5, loss 0.3076, train acc 0.888, test acc 0.840, time 15.7 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "训练修改后的模型\n",
    "'''\n",
    "lr, num_epochs, batch_size, ctx = 1.0, 5, 256, gb.try_gpu()\n",
    "net.initialize(ctx = ctx, init = init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Gluon实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size = 5),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size = 2, strides = 2),\n",
    "       nn.Conv2D(16, kernel_size = 5),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size = 2, strides = 2),\n",
    "       nn.Dense(120),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(84),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 0.6482, train acc 0.770, test acc 0.843, time 15.7 sec\n",
      "epoch 2, loss 0.4007, train acc 0.854, test acc 0.861, time 15.8 sec\n",
      "epoch 3, loss 0.3489, train acc 0.873, test acc 0.874, time 15.4 sec\n",
      "epoch 4, loss 0.3216, train acc 0.884, test acc 0.857, time 15.5 sec\n",
      "epoch 5, loss 0.3018, train acc 0.890, test acc 0.877, time 16.1 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用同样的超参数进行训练\n",
    "'''\n",
    "net.initialize(ctx = ctx, init = init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
